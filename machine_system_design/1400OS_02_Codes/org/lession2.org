理解分类的基本原理和方法.

* 2.1 Iris数据集
** 2.1.1 可视化
   可视化数据 有助于直观的认识数据. 然后扩展到更高维度、更大的数据上.
** 2.1.2 构建第一个分类模型
   最简单模型: 阈值模型
   1 留存数据和交叉验证
   如果使用训练数据进行训练, 又使用训练数据验证, 当然会得到很好的结果,但是这样实际上是循环论证是不对的.

   2 泛化能力:
   整整要得到的是 衡量模型对新样本的泛化能力. 所以应该使用未出现过的数据来评估模型的性能, 因此必须使用更严格的评估, 未出现的数据.
   得到 训练误差、测试误差.
   随着模型越来越复杂, 这些概念也越来越重要.

   3 交叉验证:
   理想情况下 我们希望在全部数据上进行训练和测试
   n折交叉验证, 将数据分成n分, 将其中一份留存, 作为测试数据.剩余的进行训练, 得到的模型应用到留存测试数据进行测试,这样对每份都做一次循环,能够循环n次得到n个近似的模型,n个对应的测试误差.然后取平均值就得到了这个模型在应用在整个数据集上的测试误差.
   通过交叉验证, 让你用整个数据集验证了你的模型效果如何, 交叉验证过程中得到的n个模型实际上都是为了进行验证你的模型, 而已经能够知道模型效果如何, 最后就直接通过整个数据集生成模型不再进行验证.
   使用训练误差评估模型 是十分不好的, 它的结果具有误导性, 我们希望在留存数据上, 或者使用交叉验证 来评价效果.
   
   
* 2.2 更复杂的分类器
  
  一个分类模型的组成:
  1 模型结构(模型本身算法)
  2 学习过程(模型的学习过程)
  3 损失函数 通过损失函数的优化 来 进行模型学习, 并且评估模型性能.
  我们可以反复使用不同的模型(实际上是这三部分), 
  损失函数 上面用过平方误差, 不同的情况需要使用不同的损失函数, 例如医疗行业, 假阴性 是很不好的这样会让患者无法得到及时治疗, 假阳性还好点, 但是代价也不小.
  那么如何构造损失函数 取决于问题情况， 通用算法 一般是把分类错误作为损失函数, 而对于某些 特定情况代价更大的问题 就要考虑综合代价, 让代价更大的分类精度更高.
  还可以尝试其他的模型.

* 2.3 更复杂的数据集 和 更复杂的分类器 

** 2.3.1 从seeds数据集中学习
   seeds 给出了多个特征的数据.
   目的是分类seeds 三个类别.
   
** 2.3.2 特征和特征工程
   *特征工程(feature engineering)*
   seeds 数据集中有一些特征并不是测量值,而是其他特征的函数映射值.
   这种通过原始特征经过函数处理得到新特征的领域 是 特征工程
   特征工程 对系统性能有很大的影响, 一个简单算法再经过很好处理的特征工程的数据上的效果往往比一个复杂模型在较差特征上的效果还好.
   *好特征* 
   *在重要的地方变化明显, 而在不重要的地方不变,就像一个信号一样, 变化特别明显说明这个地方很重要 划重点*.
   eg
   分类小麦时, 不同的小麦肯定是外观有区别, 但是往往 小麦的大小又是不定的,有的小有的大, 所以外观不能之用大小来区分, 而是需要抽象出一种 特殊形状.
   紧密度不会随大小而改变,但是会随着形状而改变. 这就是一个相当好的特征.

   一般需要背景知识 来判断什么才是一个好的特征, 例如分类狗, 一个特征就是毛发的样子, 一个是耳朵形状, 一个是花纹形状.而对于大小 重量, 并不是特别能够分类的.
   很多问题领域 已经又很多文献介绍了可能的特征 和 特征类型.
   
   *特征选择*
   我们能够自动的把好特征选择出来??, 人们已经提出了很多方法来解决这个问题。
   1) 使用PCA 主成分分析  精简特征, 并能够将特征有效去掉不重要特征.
   2) SVD 奇异值分解, 还不太会.

** 2.3.3 最邻近分类
   每个样本 实际上 就是每个特征描述. 也就是N维空间的点.
   1) 最近邻分类算法
      算法简单, 但是效果不差, 测试时可能比较耗时.
      可以通过 N 聚类, N 选的比较大, 进行训练, 将空间训练为N个类域,并先对这N个类域进行K近邻分类,得到类域的类,训练结束.
      使用模型时,将样本通过对这N个类域进行使用K近邻分类就好了 这样N肯定远远小于样本数, predict时就肯定会快了.
      可以选择N 为 样本数的1/3 1/5 甚至更小的N。
   2) 使用交叉验证方法.

   3) 归一化到无量纲.
      Z值归一化: 特征值 离 平均值有多远, 使用标准方差的个数来表示.
      标准方差: 数据离平均值的平均距离? 类似这个意思.
      1 减去均值 2 除以方差.
      features -= features.mean(axis=0)
      features /= features.std(axis =0)
      Z值归一化结果: 0为平均值, value 为标准方差的个数.
      所以经过Z值归一化 所有特征的量纲都是自己的标准方差的个数, 那么就是无量纲(取值范围差不多了)

* 2.4 二分类和多分类
  1) 阈值分类器 
     是一个简单的二类分类器
  2) K近邻
     是一个多类分类器
     
  构建二类分类器通常比解决多类分类问题更加简单.然而我们可以通过将多类问题细分为一系列二分类问题.
  最简单的方法就是 使用一系列的 一对多分类器, 对于每个可能的标签, 都有一个对应的分类器, 判断是否属于, 是就返回, 不是就循环每个分类器.
  还可以构建一个分类树, CART树 ID3树等等.分叉判断方法, 还是有所不同的 简单的ID3可以使用信息增益, 但是对于连续型数值可能需要其他方法.一般是阈值分叉方法.
  还有其他方法将二分类方法变为多分类方法, 但是没有一个是最好的,一般都不会太差.
  
* 2.5 end
  
  分类意味着对样本进行归纳, 从而构建出一个模型(能够自动对新的 未分类的数据进行分类的规则).这是机器学习的一个基础工具.

  评估模型 与 交叉验证:
  评估模型效果, 训练误差是有误导性的 过于乐观的.
  相反的, 我们必须使用 留存未训练的 测试数据来评估效果.
  交叉验证 可以不浪费样本, 又未训练样本测试.

  特征工程问题:
  选择特征、设计特征. 是机器学习的一个重要组成部分. 并且通常是能够获得最大正确率提升的重要方法.
  好的特征 往往 击败更漂亮的方法.
  后面章节有实例.

   
