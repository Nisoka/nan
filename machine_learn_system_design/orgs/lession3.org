  有标签就是有结果, 这时候通过标签解决的问题 叫有监督问题.就是对某个样本我们要得到一个期望目标.并且该目标要满足数据的内在模式.
  很多时候数据没有标签, 可能因为标注困难成本高. 这是无监督问题.无监督问题是没有目标值的, 那么能做的就是 从数据中学习到的数据满足的模式.
  数据相互之间有什么关系, 那些数据更相似, 那些数据能归类.等等.

  问答网站:
  用户访问网站搜索信息时, 我们希望尽可能返回给他特定的答案. 列出和他的问题最相关的一些历史问题.
  这就需要看 问题的相似度 和 历史问答的相似度.然后将最近似的N个帖子返回.这样子是十分耗时的. 因此我们需要一个寻找相关帖子的快速方法 --- 先聚类,再近邻.
  
* 3.1 评价相关性
  
  机器学习是数据的内在模式, 原始文本用处不大, 机器并不识别文本,更容易的方法是转化为位置+数值.
** 3.1.1 不应该怎样
** 3.1.2 应该怎样
   词袋方法
   1) 词频统计 --- 向量化.
   2) 计算相似度
   3) 聚类
   4) 确定每个帖子所在簇
   在这些步骤之前 我们还有很多工作要做.

* 3.2 预处理: 用相似的公共词语个数来衡量相似性.
     
** 3.2.1 文本转化为词袋向量
   Scikt - CountVectorizer 高效的进行词频统计并向量化.
     
** 3.2.2 统计词语
   向量化整个数据集 list[ list, list, list ]
   vectorizer.transform([new_post]) 对新帖子进行向量化
   transform 返回的词频向量 是很稀疏的, 因为词袋是统计了整个数据集中的词语. 对于每一个文本 包含的词都是词袋的一部分, 所以很稀疏.
   这样保存稀疏向量 一般使用 coo_matrix类型.
   可以通过toarray() 转化为ndarray类型.
      
** 3.2.3 词语频次向量的归一化
   归一化 一般数据集都需要记性的处理, 尤其是需要进行距离计算时.
   这里的归一化方法 不同于Z值归一化方法
   而是长度归一化.

** 3.2.4 删除不重要的词语
   有些词语 例如 most images 等等 实际上没有什么作用, 并不承载什么信息, 所以一般词袋都会删掉这些词 -- 停用词.
   stop_words
   
** 3.2.5 词干处理
   有些词语 语义相同,但是形式不同, 词袋也会将他们认为不同的词, 这样不对,应该预处理一下, 把相同意思的词语放在一起统计更有道理 -- 词干处理
   这里就需要 自然语言处理NLP 的一个NLTK工具包, 词干处理器. 预处理下数据集.
   
** 3.2.6 停用词兴奋剂
   特征的具体含义是什么？
   特征值 计时词语在帖子中出现的次数.认为值越大的特征对这个帖子越重要.
   但是有些词语eg subject 在每个帖子中几乎都出现.而且又不能简单删除,并不像极少出现的词,这些词很可能很重要的.
   对于某些词语 经常出现在一些帖子中, 而别的帖子中很少出现,我们应该给该词语更高的值, 相反得到更少的值.
   词频-反转文档频率(TF-IDF) TF-统计  IDF 考虑权重折扣
   当某个词越能分辨所属的文本时, 该词的特征值就越大.相反就越小.
   a = 'a'
   abb = 'abb'
   abc = 'abc'
   D = [a, abb, abc]
   print(tfidf("a", a, D))
   0.0
   print(tfidf("b", abb, D))
   0.27
   print(tfidf("a", abc, D))
   0.0
   print(tfidf("c", abc, D))
   0.366
   
   a 无处不在, 对于描述一个文本没什么意义
   b 对 abb 更为重要
   c 对 abc 更为重要
   对于聚类 TF-IDF十分必要.
   经过TfidfVectorizer 生成的词向量, 包含的不再是词频值, 而是TF-IDF值.
   
** 3.2.7 我们的成果和目标
   此时预处理过程包括
   1) 切分文本
   2) 扔掉高频无用词
   3) 扔掉低频无用词
   4) 词袋统计
   5) 考虑整个语料集合,词频统计计算TF-IDF值.
   经过预处理 我们将一堆充满噪声的文本转化成了一个简明的特征表示.
   注意:
   虽然词袋向量模型简单有效, 但是还是有不少缺点

* 3.3 聚类
  至此 得到了特征向量, 我们相信足以捕捉到帖子的特征.
  聚类方法一般两类 : 扁平 和 层次 聚类。
  扁平聚类 就是分成多个簇, 同一个簇内对象尽量相似, 不同簇内对象尽量不相似.
  层次聚类, 并不需要指定簇的个数.会自动构建出簇之间的层次关系....具体不知.
  Scikit sklearn.cluster 包含了聚类方法类.
** 3.3.1 KMeans 聚类
   聚类迭代 结果得到簇中心 与 标识.
   对新向量, 与簇中心进行距离判断, 归类到最近簇中.
        
** 3.3.2 测试数据 评估效果
   
** 3.3.3 聚类

* 3.4 

* 3.5 调整参数
  这是应用算法很重要 很困难的一个部分.
  可以调整不同的簇中心初始化方法
  可以调整相似度度量方法
  sklearn.metrics 可以衡量聚类质量

* 3.6 end
  预处理  
  将原始数据转化为算法可用的数据. 这是一个困难的过程,遇到各种问题,以及解决各种问题.
  
